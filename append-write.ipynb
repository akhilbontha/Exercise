{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c877f1-cc3a-4e08-9227-36e6ae68ad67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 663119 bytes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Fetch Data from the API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your API URL\n",
    "api_url = \"https://pm25.lass-net.org/API-1.0.0/device/08BEAC0AB6D6/history/\" \n",
    "response = requests.get(api_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    new_data = response.json()\n",
    "    # Ensure new_data is a list\n",
    "    if isinstance(new_data, dict):\n",
    "        new_data = [new_data]\n",
    "    elif not isinstance(new_data, list):\n",
    "        raise Exception(f\"Unexpected data format: {type(new_data)}\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "\n",
    "# Read Existing Data from DBFS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark session and DBUtils\n",
    "spark = SparkSession.builder.appName(\"APIDataStaging\").getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Define the path for the data in DBFS\n",
    "existing_data_path = \"/FileStore/tables/api_data7.json\"\n",
    "\n",
    "# Check if the file exists and read the existing data\n",
    "try:\n",
    "    existing_data_df = spark.read.json(\"dbfs:\" + existing_data_path)\n",
    "    existing_data = [row.asDict() for row in existing_data_df.collect()]\n",
    "except Exception as e:\n",
    "    # If the file does not exist, initialize an empty list\n",
    "    existing_data = []\n",
    "\n",
    "# Ensure existing_data is a list\n",
    "if not isinstance(existing_data, list):\n",
    "    existing_data = [existing_data]\n",
    "\n",
    "# Combine existing data with new data\n",
    "combined_data = existing_data + new_data\n",
    "\n",
    "# Convert combined data to JSON string\n",
    "combined_data_json = json.dumps(combined_data)\n",
    "\n",
    "# Save combined JSON data to DBFS\n",
    "dbutils.fs.put(existing_data_path, combined_data_json, overwrite=True)\n",
    "\n",
    "# Read JSON data from DBFS\n",
    "df = spark.read.json(\"dbfs:/FileStore/tables/api_data7.json\")\n",
    "\n",
    "# Show the DataFrame\n",
    "#df.show()\n",
    "\n",
    "# Print the schema\n",
    "#df.printSchema()\n",
    "\n",
    "# Doing first level flattening df1\n",
    "df1 = df.select(\"*\",explode_outer(\"feeds\").alias(\"new_feeds\")).select(\"*\",explode_outer(\"new_feeds.AirBox\").alias(\"new_AirBox\")).drop(\"new_feeds.AirBox\").drop(\"feeds\").drop(\"new_feeds\")\n",
    "\n",
    "df_1 = df1.select(\"device_id\",\"num_of_records\",\"source\",\"version\")\n",
    "\n",
    "\n",
    "df2 = df1.select(\"new_AirBox.*\")\n",
    "\n",
    "# Extract the dynamic timestamp keys\n",
    "keys = df2.schema.fieldNames()\n",
    "\n",
    "# Flatten the nested structure\n",
    "flattened_df = df2.selectExpr(\"inline(array(\" + \",\".join([f\"struct('{k}' as timestamp_key, `{k}`.*)\" for k in keys]) + \"))\")\n",
    "\n",
    "flattened_df1 = flattened_df.dropna(subset=['Date','time'])\n",
    "\n",
    "# Show the resulting flattened DataFrame\n",
    "\n",
    "df_inner = df_1.join(flattened_df1, on=\"device_id\", how=\"inner\").dropDuplicates()\n",
    "\n",
    "#display(df_inner)\n",
    "\n",
    "# Filter the DataFrame where s_d0 (PM2.5 level) is greater than 30\n",
    "df_filtered = df_inner.filter(col(\"s_d0\") > 30).select(\"date\",\"timestamp\", \"s_d0\")\n",
    "display(df_filtered)\n",
    "\n",
    "# Extract date from timestamp for daily statistics\n",
    "df = df_inner.withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Compute daily maximum, minimum, and average pollution values\n",
    "daily_stats = df.groupBy(\"date\").agg(\n",
    "    max(\"s_d0\").alias(\"daily_max\"),\n",
    "    min(\"s_d0\").alias(\"daily_min\"),\n",
    "    avg(\"s_d0\").alias(\"daily_avg\")\n",
    ")\n",
    "\n",
    "# Collect the filtered times and daily statistics\n",
    "times_above_threshold = df_filtered.collect()\n",
    "daily_stats_report = daily_stats.collect()\n",
    "\n",
    "# Display the results\n",
    "print(\"Times when PM2.5 level went above the threshold of 30:\")\n",
    "for row in times_above_threshold:\n",
    "    print(f\"Timestamp: {row['timestamp']}, PM2.5 Level: {row['s_d0']}\")\n",
    "\n",
    "print(\"\\nDaily PM2.5 Statistics:\")\n",
    "print(\"Date       | Daily Max | Daily Min | Daily Avg\")\n",
    "for row in daily_stats_report:\n",
    "    print(f\"{row['date']} | {row['daily_max']}     | {row['daily_min']}     | {row['daily_avg']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "append-write",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
